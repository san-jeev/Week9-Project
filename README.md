# Week9-Project
- The CartPole environment consists of a pole, balanced on a cart.<br>
- The agent has to learn how to balance the pole vertically, while the cart underneath it moves.<br>
- The agent is given the position of the cart, the velocity of the cart, the angle of the pole, and the rotational rate of the pole as inputs.<br>
- The agent can apply a force on either side of the cart. If the pole falls more than 15 degrees from vertical, it’s game over for our agent.<br>
- Build simple 3 hidden layers neural network model with 16 neurons each to solve the Cart-pole problem<br>
- The input is a 1 x state space vector and there will be an output neuron for each possible action that will predict the Q value of that action for each step.<br>
- By taking the argmax of the outputs, we can choose the action with the highest Q value<br>
- Print model summary<br>
- Plot the model and its Layers<br>
- We will create a DQN agent using Keras to master the CartPole-v0 environment and take several hundred episodes to eventually balance the pole<br>
- Configure appropriate CartPole-v0 Environment Variables to benchmark the ability of reinforcement learning agent<br>
- Get the environment and extract the number of actions available in the Cartpole problem<br>
- Environments are intended to have various levels of difficulty. We are going to use CartPole-v0 environment to benchmark the ability of reinforcement learning agent to solve the CartPole problem
- Create a deep Q network Agent now that we have a model and memory and policy is defined<br>
- Set policy as Epsilon Greedy and memory as Sequential Memory to store results of actions performed and rewards for each action<br>
- Use SequentialMemory is fast and efficient data structure to store agent’s experiences in; As new experiences are added to this memory and it becomes full, old experiences are forgotten.<br>
- Compile the deep Q network Agent using Adam<br>
- Use Keras-RL callbacks that allow for convenient model checkpointing and logging.<br>
- Train the Reinforcement Learning model usinf .fit(); Create training data through the trials we run and feed this information into it directly after running the trial.<br>
- Rather than training on the trials as they come in, Add them to memory and train on a random sample of that memory.<br>
- By taking a random sample, we don’t bias our training set, and instead ideally learn about scaling all environments we would encounter equally well.<br>
- Set visualize=True to watch the agent interact with the environment<br>
- Note that after the first 250 episodes, we will see that the total rewards for the episode approach 200 and the episode steps also approach 200. This means that the agent has learned to balance the pole on the cart until the environment ends at a maximum of 200 steps.<br>
- Test the Reinforcement Learning model using .test() method to evaluate for some number of episodes<br>
