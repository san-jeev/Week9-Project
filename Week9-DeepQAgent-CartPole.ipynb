{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \u001b[4mWeek 9 HW:\u001b[0m We will create a DQN agent using Keras to master the CartPole-v0 environment and take several hundred episodes to eventually balance the pole\n",
      "\n",
      "\u001b[1m \u001b[4mConfigure appropriate CartPole-v0 Environment Variables to benchmark the ability of reinforcement learning agent\u001b[0m\n",
      "\u001b[1menv= \u001b[0m<TimeLimit<CartPoleEnv<CartPole-v0>>>\n",
      "\u001b[1mnb_actions= \u001b[0m2\n",
      "\n",
      "\u001b[1m \u001b[4mFirst build simple 3 hidden layers neural network model with 16 neurons each\u001b[0m\n",
      "\n",
      "\u001b[1m \u001b[4mModel Summary\u001b[0m\n",
      "\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\u001b[1m \u001b[4mModel Layer Output Shape\u001b[0m\n",
      "\n",
      "Tensor(\"dense_8/BiasAdd:0\", shape=(?, 2), dtype=float32)\n",
      "(None, 4)\n",
      "(None, 16)\n",
      "(None, 16)\n",
      "(None, 16)\n",
      "(None, 16)\n",
      "(None, 16)\n",
      "(None, 16)\n",
      "(None, 2)\n",
      "\n",
      "\n",
      "Model Layer Output Shape layer.get_output_at(0).get_shape().as_list()\n",
      "[None, None]\n",
      "[None, 16]\n",
      "[None, 16]\n",
      "[None, 16]\n",
      "[None, 16]\n",
      "[None, 16]\n",
      "[None, 16]\n",
      "[None, 2]\n",
      "\n",
      "\n",
      "Model Layer Output Shape  l.output_shape\n",
      "(None, 4)\n",
      "(None, 16)\n",
      "(None, 16)\n",
      "(None, 16)\n",
      "(None, 16)\n",
      "(None, 16)\n",
      "(None, 16)\n",
      "(None, 2)\n",
      "\n",
      "\n",
      "\u001b[1m \u001b[4mPlot the Model and its Layers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m \u001b[4mCreate a deep Q network Agent\u001b[0m\n",
      "\n",
      "\u001b[1m \u001b[4mCompile the deep Q network Agent\u001b[0m\n",
      "\n",
      "\u001b[1m \u001b[4mUse Keras-RL callbacks for convenient model checkpointing and logging\u001b[0m\n",
      "\n",
      "\u001b[1m \u001b[4mTrain the deep Q network Agent for 5000 steps\u001b[0m\n",
      "\n",
      "Training for 5000 steps ...\n",
      "   10/5000: episode: 1, duration: 0.266s, episode steps: 10, steps per second: 38, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.967, 3.014], loss: --, mae: --, mean_q: --\n",
      "   18/5000: episode: 2, duration: 1.420s, episode steps: 8, steps per second: 6, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.178 [-1.519, 2.562], loss: 0.459044, mae: 0.597293, mean_q: 0.138447\n",
      "   28/5000: episode: 3, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.905, 3.089], loss: 0.330081, mae: 0.511736, mean_q: 0.303912\n",
      "   41/5000: episode: 4, duration: 0.103s, episode steps: 13, steps per second: 126, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.109 [-1.785, 2.852], loss: 0.220333, mae: 0.419344, mean_q: 0.541333\n",
      "   52/5000: episode: 5, duration: 0.116s, episode steps: 11, steps per second: 95, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-2.111, 3.316], loss: 0.146791, mae: 0.315221, mean_q: 0.799599\n",
      "   61/5000: episode: 6, duration: 0.080s, episode steps: 9, steps per second: 113, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.753, 2.794], loss: 0.117630, mae: 0.259873, mean_q: 1.041170\n",
      "   71/5000: episode: 7, duration: 0.081s, episode steps: 10, steps per second: 123, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.913, 3.101], loss: 0.105572, mae: 0.235393, mean_q: 1.165299\n",
      "   82/5000: episode: 8, duration: 0.093s, episode steps: 11, steps per second: 118, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.118 [-1.764, 2.754], loss: 0.085313, mae: 0.238354, mean_q: 1.248215\n",
      "   91/5000: episode: 9, duration: 0.068s, episode steps: 9, steps per second: 133, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.716, 2.815], loss: 0.081926, mae: 0.278145, mean_q: 1.300639\n",
      "  100/5000: episode: 10, duration: 0.068s, episode steps: 9, steps per second: 132, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.779, 2.894], loss: 0.058253, mae: 0.285265, mean_q: 1.401840\n",
      "  109/5000: episode: 11, duration: 0.074s, episode steps: 9, steps per second: 121, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.787, 2.754], loss: 0.051835, mae: 0.317425, mean_q: 1.486977\n",
      "  119/5000: episode: 12, duration: 0.086s, episode steps: 10, steps per second: 116, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.138 [-1.526, 2.518], loss: 0.057106, mae: 0.358879, mean_q: 1.539349\n",
      "  129/5000: episode: 13, duration: 0.076s, episode steps: 10, steps per second: 132, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.145 [-1.717, 2.672], loss: 0.058306, mae: 0.391135, mean_q: 1.628897\n",
      "  139/5000: episode: 14, duration: 0.084s, episode steps: 10, steps per second: 120, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.939, 3.082], loss: 0.045894, mae: 0.427946, mean_q: 1.685368\n",
      "  150/5000: episode: 15, duration: 0.085s, episode steps: 11, steps per second: 129, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.109 [-1.768, 2.828], loss: 0.059417, mae: 0.489861, mean_q: 1.678707\n",
      "  158/5000: episode: 16, duration: 0.062s, episode steps: 8, steps per second: 129, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.558, 2.523], loss: 0.059540, mae: 0.543517, mean_q: 1.794019\n",
      "  168/5000: episode: 17, duration: 0.083s, episode steps: 10, steps per second: 120, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.937, 3.018], loss: 0.059487, mae: 0.581676, mean_q: 1.760122\n",
      "  178/5000: episode: 18, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.965, 3.056], loss: 0.055990, mae: 0.630486, mean_q: 1.846307\n",
      "  188/5000: episode: 19, duration: 0.089s, episode steps: 10, steps per second: 112, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.173 [-1.932, 3.121], loss: 0.059540, mae: 0.674403, mean_q: 1.901957\n",
      "  197/5000: episode: 20, duration: 0.076s, episode steps: 9, steps per second: 119, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.756, 2.807], loss: 0.049543, mae: 0.698786, mean_q: 1.907913\n",
      "  207/5000: episode: 21, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.159 [-1.160, 2.070], loss: 0.060144, mae: 0.759337, mean_q: 2.000145\n",
      "  216/5000: episode: 22, duration: 0.071s, episode steps: 9, steps per second: 128, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.773, 2.813], loss: 0.061565, mae: 0.792856, mean_q: 2.044740\n",
      "  226/5000: episode: 23, duration: 0.086s, episode steps: 10, steps per second: 116, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.142 [-1.533, 2.599], loss: 0.060060, mae: 0.834958, mean_q: 2.013248\n",
      "  235/5000: episode: 24, duration: 0.069s, episode steps: 9, steps per second: 130, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.158 [-1.746, 2.798], loss: 0.063699, mae: 0.889975, mean_q: 2.121959\n",
      "  246/5000: episode: 25, duration: 0.088s, episode steps: 11, steps per second: 125, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.109 [-1.775, 2.775], loss: 0.049782, mae: 0.920127, mean_q: 2.159426\n",
      "  254/5000: episode: 26, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.551, 2.549], loss: 0.053559, mae: 0.972661, mean_q: 2.226405\n",
      "  264/5000: episode: 27, duration: 0.079s, episode steps: 10, steps per second: 127, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-1.964, 3.004], loss: 0.044337, mae: 1.018898, mean_q: 2.301349\n",
      "  273/5000: episode: 28, duration: 0.071s, episode steps: 9, steps per second: 127, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.810, 2.873], loss: 0.055863, mae: 1.068814, mean_q: 2.311086\n",
      "  283/5000: episode: 29, duration: 0.084s, episode steps: 10, steps per second: 120, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [-1.910, 3.113], loss: 0.048786, mae: 1.108041, mean_q: 2.415807\n",
      "  292/5000: episode: 30, duration: 0.072s, episode steps: 9, steps per second: 124, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.717, 2.796], loss: 0.041939, mae: 1.126211, mean_q: 2.388810\n",
      "  302/5000: episode: 31, duration: 0.079s, episode steps: 10, steps per second: 127, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.985, 3.032], loss: 0.064213, mae: 1.215307, mean_q: 2.533394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  311/5000: episode: 32, duration: 0.084s, episode steps: 9, steps per second: 107, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.807, 2.880], loss: 0.047379, mae: 1.240178, mean_q: 2.545115\n",
      "  321/5000: episode: 33, duration: 0.083s, episode steps: 10, steps per second: 120, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.131 [-1.544, 2.503], loss: 0.058686, mae: 1.287100, mean_q: 2.570323\n",
      "  330/5000: episode: 34, duration: 0.068s, episode steps: 9, steps per second: 133, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.151 [-1.516, 2.495], loss: 0.048092, mae: 1.366330, mean_q: 2.747464\n",
      "  338/5000: episode: 35, duration: 0.072s, episode steps: 8, steps per second: 112, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.590, 2.565], loss: 0.053087, mae: 1.339788, mean_q: 2.562107\n",
      "  348/5000: episode: 36, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.129 [-1.781, 2.727], loss: 0.057546, mae: 1.411255, mean_q: 2.725542\n",
      "  358/5000: episode: 37, duration: 0.079s, episode steps: 10, steps per second: 126, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.121 [-1.545, 2.343], loss: 0.044183, mae: 1.440354, mean_q: 2.765671\n",
      "  405/5000: episode: 38, duration: 0.402s, episode steps: 47, steps per second: 117, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.007 [-0.989, 0.600], loss: 0.038419, mae: 1.578779, mean_q: 3.023443\n",
      "  419/5000: episode: 39, duration: 0.135s, episode steps: 14, steps per second: 103, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.103 [-1.238, 0.597], loss: 0.043507, mae: 1.684316, mean_q: 3.218480\n",
      "  432/5000: episode: 40, duration: 0.138s, episode steps: 13, steps per second: 95, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.106 [-1.931, 1.165], loss: 0.034279, mae: 1.722970, mean_q: 3.320210\n",
      "  443/5000: episode: 41, duration: 0.106s, episode steps: 11, steps per second: 104, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.121 [-3.302, 2.151], loss: 0.077288, mae: 1.813558, mean_q: 3.488560\n",
      "  455/5000: episode: 42, duration: 0.120s, episode steps: 12, steps per second: 100, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.137 [-3.094, 1.908], loss: 0.148141, mae: 1.878509, mean_q: 3.559611\n",
      "  465/5000: episode: 43, duration: 0.096s, episode steps: 10, steps per second: 105, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.099 [-2.945, 1.990], loss: 0.089742, mae: 2.018894, mean_q: 3.920751\n",
      "  474/5000: episode: 44, duration: 0.090s, episode steps: 9, steps per second: 100, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.830, 1.801], loss: 0.090829, mae: 2.012922, mean_q: 3.919627\n",
      "  485/5000: episode: 45, duration: 0.111s, episode steps: 11, steps per second: 99, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.121 [-2.813, 1.802], loss: 0.255034, mae: 2.022252, mean_q: 3.809922\n",
      "  495/5000: episode: 46, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.097 [-2.412, 1.600], loss: 0.141132, mae: 1.990766, mean_q: 3.762019\n",
      "  505/5000: episode: 47, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.112, 1.995], loss: 0.161643, mae: 2.137614, mean_q: 4.065217\n",
      "  515/5000: episode: 48, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.170 [-3.107, 1.923], loss: 0.217110, mae: 2.092995, mean_q: 3.908623\n",
      "  525/5000: episode: 49, duration: 0.082s, episode steps: 10, steps per second: 122, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.103, 1.949], loss: 0.099822, mae: 2.124238, mean_q: 4.114325\n",
      "  534/5000: episode: 50, duration: 0.074s, episode steps: 9, steps per second: 121, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.170 [-2.857, 1.741], loss: 0.100711, mae: 2.158106, mean_q: 4.174095\n",
      "  545/5000: episode: 51, duration: 0.094s, episode steps: 11, steps per second: 117, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.116 [-2.274, 1.408], loss: 0.130161, mae: 2.108874, mean_q: 3.983033\n",
      "  555/5000: episode: 52, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.130 [-2.034, 1.147], loss: 0.142980, mae: 2.216455, mean_q: 4.214884\n",
      "  563/5000: episode: 53, duration: 0.064s, episode steps: 8, steps per second: 125, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.526, 1.526], loss: 0.152771, mae: 2.348453, mean_q: 4.475034\n",
      "  573/5000: episode: 54, duration: 0.084s, episode steps: 10, steps per second: 119, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-3.135, 1.952], loss: 0.153947, mae: 2.351631, mean_q: 4.486116\n",
      "  583/5000: episode: 55, duration: 0.081s, episode steps: 10, steps per second: 123, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.083, 1.975], loss: 0.147553, mae: 2.328579, mean_q: 4.460692\n",
      "  596/5000: episode: 56, duration: 0.098s, episode steps: 13, steps per second: 133, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.125 [-2.856, 1.734], loss: 0.238379, mae: 2.387247, mean_q: 4.502030\n",
      "  606/5000: episode: 57, duration: 0.085s, episode steps: 10, steps per second: 117, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.091, 1.942], loss: 0.117360, mae: 2.411769, mean_q: 4.626027\n",
      "  617/5000: episode: 58, duration: 0.084s, episode steps: 11, steps per second: 131, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.112 [-2.774, 1.790], loss: 0.165425, mae: 2.470241, mean_q: 4.724223\n",
      "  627/5000: episode: 59, duration: 0.077s, episode steps: 10, steps per second: 129, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-2.966, 1.915], loss: 0.091878, mae: 2.458462, mean_q: 4.757863\n",
      "  637/5000: episode: 60, duration: 0.088s, episode steps: 10, steps per second: 114, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.138 [-2.559, 1.527], loss: 0.218601, mae: 2.489128, mean_q: 4.704944\n",
      "  647/5000: episode: 61, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.167 [-3.136, 1.944], loss: 0.217247, mae: 2.557502, mean_q: 4.741094\n",
      "  656/5000: episode: 62, duration: 0.072s, episode steps: 9, steps per second: 126, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.818, 1.767], loss: 0.229629, mae: 2.502671, mean_q: 4.707172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  664/5000: episode: 63, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.134 [-2.015, 1.219], loss: 0.145413, mae: 2.541965, mean_q: 4.859908\n",
      "  679/5000: episode: 64, duration: 0.117s, episode steps: 15, steps per second: 128, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.098 [-2.892, 1.806], loss: 0.202802, mae: 2.578806, mean_q: 4.892703\n",
      "  687/5000: episode: 65, duration: 0.068s, episode steps: 8, steps per second: 118, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.523, 1.552], loss: 0.163127, mae: 2.685857, mean_q: 5.146808\n",
      "  696/5000: episode: 66, duration: 0.079s, episode steps: 9, steps per second: 113, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.838, 1.779], loss: 0.145636, mae: 2.649038, mean_q: 5.111555\n",
      "  704/5000: episode: 67, duration: 0.066s, episode steps: 8, steps per second: 121, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.524, 1.562], loss: 0.197945, mae: 2.595789, mean_q: 4.948009\n",
      "  714/5000: episode: 68, duration: 0.076s, episode steps: 10, steps per second: 132, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-3.051, 1.976], loss: 0.155616, mae: 2.634408, mean_q: 5.066549\n",
      "  723/5000: episode: 69, duration: 0.075s, episode steps: 9, steps per second: 119, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.835, 1.775], loss: 0.107320, mae: 2.654429, mean_q: 5.142488\n",
      "  735/5000: episode: 70, duration: 0.092s, episode steps: 12, steps per second: 131, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.102 [-2.058, 1.197], loss: 0.173653, mae: 2.727013, mean_q: 5.210686\n",
      "  745/5000: episode: 71, duration: 0.080s, episode steps: 10, steps per second: 125, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-3.011, 1.909], loss: 0.161592, mae: 2.748085, mean_q: 5.257807\n",
      "  755/5000: episode: 72, duration: 0.083s, episode steps: 10, steps per second: 121, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.117 [-2.966, 1.983], loss: 0.091094, mae: 2.808990, mean_q: 5.442132\n",
      "  766/5000: episode: 73, duration: 0.085s, episode steps: 11, steps per second: 129, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.140 [-2.270, 1.330], loss: 0.182128, mae: 2.772296, mean_q: 5.272211\n",
      "  776/5000: episode: 74, duration: 0.077s, episode steps: 10, steps per second: 131, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.111 [-2.987, 1.988], loss: 0.121985, mae: 2.847593, mean_q: 5.449328\n",
      "  786/5000: episode: 75, duration: 0.080s, episode steps: 10, steps per second: 125, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-3.102, 1.977], loss: 0.102637, mae: 2.747739, mean_q: 5.271529\n",
      "  868/5000: episode: 76, duration: 0.609s, episode steps: 82, steps per second: 135, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.549 [0.000, 1.000], mean observation: 0.172 [-2.134, 1.593], loss: 0.123560, mae: 2.915250, mean_q: 5.555730\n",
      "  876/5000: episode: 77, duration: 0.063s, episode steps: 8, steps per second: 127, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.563, 1.591], loss: 0.131085, mae: 2.982259, mean_q: 5.710087\n",
      "  886/5000: episode: 78, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.114 [-2.679, 1.799], loss: 0.086087, mae: 3.047480, mean_q: 5.820750\n",
      "  907/5000: episode: 79, duration: 0.164s, episode steps: 21, steps per second: 128, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.074 [-1.472, 1.011], loss: 0.087617, mae: 3.080383, mean_q: 5.941720\n",
      "  961/5000: episode: 80, duration: 0.407s, episode steps: 54, steps per second: 133, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.138 [-1.029, 0.253], loss: 0.106028, mae: 3.293469, mean_q: 6.361277\n",
      " 1074/5000: episode: 81, duration: 0.826s, episode steps: 113, steps per second: 137, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.194 [-1.399, 1.895], loss: 0.146785, mae: 3.663978, mean_q: 7.107923\n",
      " 1221/5000: episode: 82, duration: 1.248s, episode steps: 147, steps per second: 118, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.452 [-2.252, 0.695], loss: 0.133964, mae: 4.130222, mean_q: 8.077334\n",
      " 1421/5000: episode: 83, duration: 1.506s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.208 [-1.267, 0.577], loss: 0.158342, mae: 4.810824, mean_q: 9.541947\n",
      " 1621/5000: episode: 84, duration: 1.532s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.147 [-1.082, 0.766], loss: 0.195807, mae: 5.672001, mean_q: 11.311560\n",
      " 1821/5000: episode: 85, duration: 1.701s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.170 [-1.139, 0.557], loss: 0.250513, mae: 6.549687, mean_q: 13.121310\n",
      " 2021/5000: episode: 86, duration: 2.338s, episode steps: 200, steps per second: 86, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.204 [-1.360, 0.512], loss: 0.266830, mae: 7.433414, mean_q: 14.906826\n",
      " 2221/5000: episode: 87, duration: 2.514s, episode steps: 200, steps per second: 80, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.258 [-1.596, 0.669], loss: 0.499270, mae: 8.215215, mean_q: 16.457130\n",
      " 2421/5000: episode: 88, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.267 [-1.770, 0.562], loss: 0.446751, mae: 9.041965, mean_q: 18.144173\n",
      " 2621/5000: episode: 89, duration: 1.957s, episode steps: 200, steps per second: 102, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.249 [-1.602, 0.728], loss: 0.610782, mae: 9.757624, mean_q: 19.614107\n",
      " 2821/5000: episode: 90, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.357 [-2.275, 0.432], loss: 0.492774, mae: 10.473350, mean_q: 21.095993\n",
      " 3021/5000: episode: 91, duration: 2.764s, episode steps: 200, steps per second: 72, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.362 [-2.378, 0.402], loss: 0.550543, mae: 11.178830, mean_q: 22.543633\n",
      " 3221/5000: episode: 92, duration: 2.809s, episode steps: 200, steps per second: 71, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.240 [-1.586, 0.603], loss: 0.456495, mae: 11.839120, mean_q: 23.861732\n",
      " 3421/5000: episode: 93, duration: 2.919s, episode steps: 200, steps per second: 69, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.151 [-0.960, 0.493], loss: 0.793753, mae: 12.604057, mean_q: 25.394522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3621/5000: episode: 94, duration: 3.129s, episode steps: 200, steps per second: 64, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.244 [-1.535, 0.593], loss: 0.872054, mae: 13.261531, mean_q: 26.730209\n",
      " 3821/5000: episode: 95, duration: 2.116s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.073 [-0.681, 0.536], loss: 0.909013, mae: 13.966487, mean_q: 28.116804\n",
      " 4021/5000: episode: 96, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.175 [-1.053, 0.520], loss: 1.138288, mae: 14.651153, mean_q: 29.496588\n",
      " 4221/5000: episode: 97, duration: 2.428s, episode steps: 200, steps per second: 82, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.327 [-2.115, 0.550], loss: 1.190451, mae: 15.232090, mean_q: 30.637463\n",
      " 4421/5000: episode: 98, duration: 1.878s, episode steps: 200, steps per second: 106, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.139 [-0.930, 0.611], loss: 1.038740, mae: 15.862754, mean_q: 32.007092\n",
      " 4621/5000: episode: 99, duration: 2.009s, episode steps: 200, steps per second: 100, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.074 [-0.760, 0.513], loss: 1.087208, mae: 16.443367, mean_q: 33.180214\n",
      " 4821/5000: episode: 100, duration: 1.691s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.165 [-1.076, 0.494], loss: 1.546516, mae: 17.148151, mean_q: 34.569111\n",
      "done, took 52.260 seconds\n",
      "\n",
      "\u001b[1m \u001b[4mNote:\u001b[0m After the first 250 episodes, we will see that the total rewards for the episode approach 200 and the episode steps also approach 200. This means that the agent has learned to balance the pole on the cart until the environment ends at a maximum of 200 steps.\n",
      "\n",
      "\u001b[1m \u001b[4mTest the deep Q network Agent for 5 Episodes\u001b[0m\n",
      "\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f523ce35c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Week 9 Homework \n",
    "# Develop and train a Keras-RL Reinforcement learning agent called CartPole. \n",
    "# The CartPole environment consists of a pole, balanced on a cart. \n",
    "# The deep Q agent has to learn how to balance the pole vertically, while the cart underneath it moves. \n",
    "# The agent is given the position of the cart, the velocity of the cart, the angle of the pole, and the rotational rate of the pole as inputs. \n",
    "# The agent can apply a force on either side of the cart. If the pole falls more than 15 degrees from vertical, it’s game over for our agent.\n",
    "\n",
    "# Import dependencies\n",
    "import gym # OpenAI gym is a toolkit for developing and comparing reinforcement learning algorithms.\n",
    "from gym import wrappers # gym wrapper call to save .mp4 files to ./videos/1234/cartpole.mp4\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import model_to_dot\n",
    "from keras.callbacks import Callback as KerasCallback\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import pydot\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # ignore warnings\n",
    "import tensorflow as tf\n",
    "from time import time # to have timestamps in the CarPole video file\n",
    "\n",
    "# !pip install keras-rl\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "print('\\033[1m \\033[4mWeek 9 HW:\\033[0m' ' We will create a DQN agent using Keras to master the CartPole-v0 environment and take several hundred episodes to eventually balance the pole' '\\n')\n",
    "\n",
    "# Set Parameters, Environment Variables\n",
    "print('\\033[1m \\033[4mConfigure appropriate CartPole-v0 Environment Variables to benchmark the ability of reinforcement learning agent\\033[0m')\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "# Environments are intended to have various levels of difficulty. We are going to use CartPole-v0 environment to benchmark the ability of reinforcement learning agent to solve the CartPole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "env_wrapper = wrappers.Monitor(env, './videos/' + str(time()) + '/')  # Insert a gym wrapper call after you make the env to save .mp4 files to ./videos/1234/something.mp4\n",
    "print('\\033[1m' 'env= ' '\\033[0m' + str(env))\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "print('\\033[1m' 'nb_actions= ' '\\033[0m' + str(nb_actions) + '\\n')\n",
    "\n",
    "# First build a simple 3 hidden layers neural network model with 16 neurons each that will later learn through reinforcement learning and solve the Cart-pole problem\n",
    "# The input is a 1 x state space vector and there will be an output neuron for each possible action that will predict the Q value of that action for each step. \n",
    "# By taking the argmax of the outputs, we can choose the action with the highest Q value \n",
    "print('\\033[1m \\033[4mFirst build simple 3 hidden layers neural network model with 16 neurons each\\033[0m' '\\n')\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "print('\\033[1m \\033[4mModel Summary\\033[0m' '\\n')\n",
    "print(model.summary())\n",
    "print(\"\")\n",
    "\n",
    "print('\\033[1m \\033[4mModel Layer Output Shape\\033[0m' '\\n')\n",
    "print(model.output)\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.output_shape)\n",
    "\n",
    "print('\\n')\n",
    "print('Model Layer Output Shape layer.get_output_at(0).get_shape().as_list()')\n",
    "for layer in model.layers:\n",
    "    print(layer.get_output_at(0).get_shape().as_list())\n",
    "    \n",
    "print('\\n')\n",
    "print('Model Layer Output Shape  l.output_shape')\n",
    "for l in model.layers:\n",
    "    print (l.output_shape)\n",
    "print('\\n')\n",
    "      \n",
    "# Plot the Model and its layers\n",
    "print('\\033[1m \\033[4mPlot the Model and its Layers\\033[0m' '\\n')\n",
    "path = F\"/content/gdrive/My Drive\" \n",
    "plot_model(model, to_file='DQN-model.png')\n",
    "model_img = Image.open(\"DQN-model.png\")\n",
    "plt.figure(1, figsize = (16 , 16))\n",
    "plt.title('Model')\n",
    "plt.imshow(model_img)\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "# Create a deep Q network Agent now that we have a model and memory and policy is defined \n",
    "print('\\033[1m \\033[4mCreate a deep Q network Agent\\033[0m' '\\n')\n",
    "\n",
    "# Set policy as Epsilon Greedy and memory as Sequential Memory to store results of actions performed and rewards for each action\n",
    "policy = EpsGreedyQPolicy() # greedy Q Policy is used to balance exploration and exploitation. \n",
    "\n",
    "# SequentialMemory is fast and efficient data structure to store agent’s experiences in; As new experiences are added to this memory and it becomes full, old experiences are forgotten.\n",
    "memory = SequentialMemory(limit=50000, window_length=1) \n",
    "\n",
    "# nb_steps_warmup: Determines how long we wait before we start doing experience replay, which if you recall, is when we actually start training the network. This lets us build up enough experience to build a proper minibatch. If you choose a value for this parameter that’s smaller than your batch size, Keras RL will sample with a replacement.\n",
    "# target_model_update: The Q function is recursive and when the agent updates it’s network for Q(s,a) that update also impacts the prediction it will make for Q(s’, a). This can make for a very unstable network. The way most deep Q network implementations address this limitation is by using a target network, which is a copy of the deep Q network that isn’t trained, but rather replaced with a fresh copy every so often. The target_model_update parameter controls how often this happens.\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n",
    "\n",
    "# Compile the deep Q network Agent using Adam (adaptive learning rate optimization algorithm) that's been designed specifically for training deep neural networks. \n",
    "# Adam leverages the power of adaptive learning rates methods to find individual learning rates for each parameter.\n",
    "print('\\033[1m \\033[4mCompile the deep Q network Agent\\033[0m' '\\n')\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Keras-RL provides several Keras-like callbacks that allow for convenient model checkpointing and logging. \n",
    "# We will use both of those callbacks below.\n",
    "class Callback(KerasCallback):\n",
    "    def _set_env(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "class ModelIntervalCheckpoint(Callback):\n",
    "    def __init__(self, filepath, interval, verbose=0):\n",
    "        super(ModelIntervalCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.interval = interval\n",
    "        self.verbose = verbose\n",
    "        self.total_steps = 0\n",
    "        \n",
    "class FileLogger(Callback):\n",
    "    def __init__(self, filepath, interval=None):\n",
    "        self.filepath = filepath\n",
    "        self.interval = interval\n",
    "\n",
    "        # Some algorithms compute multiple episodes at once since they are multi-threaded.\n",
    "        # We therefore use a dict that maps from episode to metrics array.\n",
    "        self.metrics = {}\n",
    "        self.starts = {}\n",
    "        self.data = {}\n",
    "        \n",
    "def build_callbacks(env_name):\n",
    "    checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "    log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=5000)]\n",
    "    callbacks += [FileLogger(log_filename, interval=100)]\n",
    "    return callbacks\n",
    "\n",
    "print('\\033[1m \\033[4mUse Keras-RL callbacks for convenient model checkpointing and logging\\033[0m' '\\n')\n",
    "callbacks = build_callbacks(ENV_NAME)\n",
    "\n",
    "# Train the Reinforcement Learning model usinf .fit(); Create training data through the trials we run and feed this information into it directly after running the trial. \n",
    "# Rather than training on the trials as they come in, we add them to memory and train on a random sample of that memory.\n",
    "# By taking a random sample, we don’t bias our training set, and instead ideally learn about scaling all environments we would encounter equally well.\n",
    "print('\\033[1m \\033[4mTrain the deep Q network Agent for 5000 steps\\033[0m' '\\n')\n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2, callbacks=callbacks) # set visualize=True to watch the agent interact with the environment\n",
    "\n",
    "print(\"\")\n",
    "print('\\033[1m \\033[4mNote:\\033[0m' ' After the first 250 episodes, we see that the total rewards for the episode approach 200 and the episode steps also approach 200. This means that the agent has learned to balance the pole on the cart until the environment ends at a maximum of 200 steps.')\n",
    "print(\"\")\n",
    "\n",
    "# Test the Reinforcement Learning model using .test() method to evaluate for some number of episodes \n",
    "print('\\033[1m \\033[4mTest the deep Q network Agent for 5 Episodes\\033[0m' '\\n')\n",
    "dqn.test(env, nb_episodes=5, visualize=True) # set visualize=True to watch agent balance the pole\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
